"""
HYBRID TRAINING - MOBILENETV2 PARTIAL FREEZE
Ph∆∞∆°ng ph√°p: ƒê√≥ng bƒÉng backbone, unfreeze v√†i block cu·ªëi + train classifier
M·ª•c ƒë√≠ch: C√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô training v√† accuracy
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import models
import time
import random
from contextlib import nullcontext  # D√πng ƒë·ªÉ handle context manager khi kh√¥ng d√πng AMP
from tqdm import tqdm  # Progress bar ƒë·∫πp
from torch.amp import autocast, GradScaler  # Mixed Precision Training
from data import Cv2PreprocessDataset, transform_config

# ============================================================
# PH·∫¶N 1: C·∫§U H√åNH GLOBAL
# L√Ω do ƒë·∫∑t ngo√†i: Windows multiprocessing c·∫ßn import l·∫°i file
# ‚Üí Bi·∫øn ph·∫£i ·ªü global scope ƒë·ªÉ workers th·∫•y ƒë∆∞·ª£c
# ============================================================

# --- 1.1. Hardware Config ---
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
# Gi·∫£i th√≠ch: 
# - torch.cuda.is_available(): Ki·ªÉm tra c√≥ GPU NVIDIA + CUDA kh√¥ng
# - N·∫øu C√ì: d√πng 'cuda' (nhanh g·∫•p 10-50 l·∫ßn)
# - N·∫øu KH√îNG: d√πng 'cpu' (ch·∫≠m nh∆∞ng v·∫´n ch·∫°y ƒë∆∞·ª£c)

# --- 1.2. Data Loading Config ---
BATCH_SIZE = 32
# Gi·∫£i th√≠ch:
# - M·ªói l·∫ßn ƒë∆∞a 32 ·∫£nh v√†o GPU ƒë·ªÉ train
# - T·∫°i sao 32? V·ª´a ƒë·ªß cho GPU 4GB, v·ª´a ƒë·ªß ƒë·ªÉ gradient ·ªïn ƒë·ªãnh
# - Nh·ªè h∆°n (8-16): Ch·∫≠m, nh∆∞ng √≠t VRAM
# - L·ªõn h∆°n (64-128): Nhanh, nh∆∞ng c·∫ßn GPU m·∫°nh

NUM_WORKERS = 4
# Gi·∫£i th√≠ch:
# - S·ªë thread CPU load ·∫£nh song song (kh√¥ng block GPU)
# - 0: Main thread load (ch·∫≠m, GPU ph·∫£i ch·ªù)
# - 4: 4 threads load song song ‚Üí GPU ch·∫°y li√™n t·ª•c
# - Rule: cpu_count // 2, max 8
# - L∆∞u √Ω: Windows c·∫ßn if __name__ guard ƒë·ªÉ kh√¥ng b·ªã l·ªói

NUM_EPOCHS = 50
# Gi·∫£i th√≠ch:
# - S·ªë l·∫ßn model "nh√¨n" to√†n b·ªô dataset
# - 1 epoch = 1 l·∫ßn qu√©t h·∫øt t·∫•t c·∫£ ·∫£nh training
# - 50 epochs: ƒê·ªß ƒë·ªÉ model h·ªçc t·ªët, c√≥ early stopping n√™n c√≥ th·ªÉ d·ª´ng s·ªõm

# --- 1.3. Learning Rate Config ---
HEAD_LR = 1e-3  # = 0.001
# Gi·∫£i th√≠ch:
# - Learning rate cho classifier (head) - ph·∫ßn M·ªöI c·∫ßn h·ªçc nhi·ªÅu
# - 1e-3 = 0.001: T·ªëc ƒë·ªô h·ªçc v·ª´a ph·∫£i
# - Qu√° cao (0.1): Model "nh·∫£y lung tung", kh√¥ng h·ªôi t·ª•
# - Qu√° th·∫•p (0.00001): H·ªçc qu√° ch·∫≠m, t·ªën th·ªùi gian

BACKBONE_LR = 1e-4  # = 0.0001
# Gi·∫£i th√≠ch:
# - Learning rate cho backbone (features) - ph·∫ßn ƒê√É TRAIN S·∫¥N
# - Th·∫•p h∆°n HEAD_LR g·∫•p 10 l·∫ßn v√¨ backbone ƒë√£ h·ªçc t·ªët r·ªìi
# - Ch·ªâ c·∫ßn "tinh ch·ªânh" nh·∫π, kh√¥ng mu·ªën ph√° h·ªèng ki·∫øn th·ª©c c≈©

WEIGHT_DECAY = 1e-4  # = 0.0001
# Gi·∫£i th√≠ch:
# - L2 regularization - "ph·∫°t" weights qu√° l·ªõn
# - C√¥ng th·ª©c: loss_total = loss + weight_decay * sum(w¬≤)
# - M·ª•c ƒë√≠ch: Tr√°nh overfitting, model t·ªïng qu√°t h∆°n
# - 1e-4: Gi√° tr·ªã chu·∫©n cho transfer learning

# --- 1.4. Model Config ---
UNFREEZE_LAST_N_BLOCKS = 2
# Gi·∫£i th√≠ch:
# - MobileNetV2 c√≥ 17 blocks (features[0] ƒë·∫øn features[16])
# - M·ªü kh√≥a 2 blocks CU·ªêI (features[15], features[16]) ƒë·ªÉ h·ªçc
# - T·∫°i sao? Blocks cu·ªëi h·ªçc "high-level features" g·∫ßn v·ªõi task
# - Trade-off:
#   * Unfreeze 0: Nhanh nh·∫•t, accuracy ~85%
#   * Unfreeze 2: Ch·∫≠m h∆°n ch√∫t, accuracy ~88% ‚Üê D√ôNG
#   * Unfreeze 17: Ch·∫≠m nh·∫•t, accuracy ~90%, d·ªÖ overfit

NUM_CLASSES = 4
# Gi·∫£i th√≠ch:
# - S·ªë l·ªõp c·∫ßn ph√¢n lo·∫°i: CR, LP, OK, PO
# - ImageNet g·ªëc: 1000 classes
# - Ch√∫ng ta: 4 classes ‚Üí Thay classifier layer

DATA_PATH = r'C:\project\picture-hust\data\train'
# Gi·∫£i th√≠ch:
# - r'...' : Raw string, tr√°nh l·ªói v·ªõi backslash \ tr√™n Windows
# - ƒê∆∞·ªùng d·∫´n ƒë·∫øn folder ch·ª©a ·∫£nh training

# ============================================================
# PH·∫¶N 2: MAIN GUARD - B·∫ÆT BU·ªòC CHO MULTIPROCESSING
# L√Ω do: Tr√°nh "recursive spawn" tr√™n Windows
# ============================================================
if __name__ == '__main__':
    # Gi·∫£i th√≠ch if __name__ == '__main__':
    # - Khi ch·∫°y: python train.py ‚Üí __name__ = '__main__' ‚Üí V√†o ƒë√¢y
    # - Khi import: import train ‚Üí __name__ = 'train' ‚Üí KH√îNG v√†o
    # - Windows spawn workers ‚Üí import l·∫°i file ‚Üí Kh√¥ng t·∫°o DataLoader l·∫°i
    # ‚Üí Tr√°nh ƒë·ªá quy v√¥ h·∫°n!
    
    # ============================================================
    # PH·∫¶N 2.1: REPRODUCIBILITY - ƒê·∫¢M B·∫¢O K·∫æT QU·∫¢ L·∫∂P L·∫†I ƒê∆Ø·ª¢C
    # ============================================================
    SEED = 42
    # Gi·∫£i th√≠ch:
    # - Seed = "h·∫°t gi·ªëng" cho random number generator
    # - D√πng c√πng seed ‚Üí c√πng k·∫øt qu·∫£ random ‚Üí k·∫øt qu·∫£ l·∫∑p l·∫°i ƒë∆∞·ª£c
    # - 42: S·ªë ph·ªï bi·∫øn (t·ª´ "The Hitchhiker's Guide to the Galaxy")
    
    random.seed(SEED)
    # Gi·∫£i th√≠ch:
    # - Set seed cho module random c·ªßa Python (shuffle, random.choice...)
    
    torch.manual_seed(SEED)
    # Gi·∫£i th√≠ch:
    # - Set seed cho PyTorch CPU operations
    # - ·∫¢nh h∆∞·ªüng: weight initialization, dropout masks...
    
    torch.cuda.manual_seed_all(SEED)
    # Gi·∫£i th√≠ch:
    # - Set seed cho T·∫§T C·∫¢ GPU (n·∫øu c√≥ nhi·ªÅu GPU)
    # - ƒê·∫£m b·∫£o k·∫øt qu·∫£ gi·ªëng nhau tr√™n m·ªçi GPU

    print(f"üî• Hardware: {DEVICE} | Workers: {NUM_WORKERS}")
    # In ra th√¥ng tin hardware ƒë·ªÉ bi·∫øt ƒëang train tr√™n g√¨
    
    # ============================================================
    # PH·∫¶N 3: CHU·∫®N B·ªä D·ªÆ LI·ªÜU
    # Flow: Load ‚Üí Split ‚Üí Separate Transforms ‚Üí DataLoader
    # ============================================================
    print("üìÇ ƒêang ƒë·ªçc d·ªØ li·ªáu...")
    
    # --- 3.1. Load Dataset ---
    full_ds = Cv2PreprocessDataset(DATA_PATH, transform=None)
    # Gi·∫£i th√≠ch:
    # - Load to√†n b·ªô dataset KH√îNG c√≥ transform
    # - T·∫°i sao None? V√¨ train/val c·∫ßn transform KH√ÅC NHAU
    # - Train: Augmentation (flip, rotate...)
    # - Val: Ch·ªâ resize + normalize (kh√¥ng augment)
    
    # --- 3.2. Train/Val Split ---
    train_size = int(0.8 * len(full_ds))
    # Gi·∫£i th√≠ch:
    # - 80% cho training
    # - int(): L√†m tr√≤n xu·ªëng (v√≠ d·ª•: 0.8 * 1000 = 800)
    
    val_size = len(full_ds) - train_size
    # Gi·∫£i th√≠ch:
    # - 20% c√≤n l·∫°i cho validation
    # - D√πng ph√©p tr·ª´ ƒë·ªÉ ƒë·∫£m b·∫£o train_size + val_size = total
    
    split_gen = torch.Generator().manual_seed(SEED)
    # Gi·∫£i th√≠ch:
    # - T·∫°o random generator ri√™ng cho vi·ªác split
    # - D√πng SEED ƒë·ªÉ ƒë·∫£m b·∫£o m·ªói l·∫ßn ch·∫°y, split gi·ªëng nhau
    # - Quan tr·ªçng: Val set ph·∫£i gi·ªØ nguy√™n ƒë·ªÉ so s√°nh c√°c l·∫ßn train
    
    train_indices, val_indices = random_split(
        range(len(full_ds)),
        [train_size, val_size],
        generator=split_gen
    )
    # Gi·∫£i th√≠ch:
    # - random_split(): Chia ng·∫´u nhi√™n indices
    # - range(len(full_ds)): [0, 1, 2, ..., 999] (n·∫øu 1000 ·∫£nh)
    # - [train_size, val_size]: [800, 200]
    # - generator: D√πng seed ƒë√£ set
    # ‚Üí train_indices: [543, 12, 789, ...]  (800 s·ªë)
    # ‚Üí val_indices:   [45, 234, 678, ...]  (200 s·ªë)
    
    # --- 3.3. Create Separate Datasets v·ªõi Transforms Kh√°c Nhau ---
    train_ds = Cv2PreprocessDataset(DATA_PATH, transform=transform_config['train'])
    # Gi·∫£i th√≠ch:
    # - T·∫°o dataset cho TRAIN v·ªõi augmentation
    # - transform_config['train']: flip, rotate, brightness...
    # - M·ª•c ƒë√≠ch: TƒÉng ƒë·ªô ƒëa d·∫°ng data ‚Üí model t·ªïng qu√°t h∆°n
    
    val_ds = Cv2PreprocessDataset(DATA_PATH, transform=transform_config['val'])
    # Gi·∫£i th√≠ch:
    # - T·∫°o dataset cho VAL KH√îNG c√≥ augmentation
    # - transform_config['val']: ch·ªâ resize + normalize
    # - T·∫°i sao kh√¥ng augment? Mu·ªën ƒë√°nh gi√° ƒê√öNG kh·∫£ nƒÉng model
    
    train_ds.samples = [full_ds.samples[i] for i in train_indices]
    # Gi·∫£i th√≠ch:
    # - G√°n l·∫°i samples c·ªßa train_ds = subset t·ª´ full_ds
    # - full_ds.samples: List[(img_path, label), ...]
    # - train_indices: [543, 12, 789, ...]
    # ‚Üí train_ds.samples = [full_ds.samples[543], full_ds.samples[12], ...]
    
    val_ds.samples = [full_ds.samples[i] for i in val_indices]
    # T∆∞∆°ng t·ª± cho val set

    # --- 3.4. Check CUDA availability cho t·ªëi ∆∞u DataLoader ---
    use_cuda = (DEVICE == 'cuda')
    # Gi·∫£i th√≠ch:
    # - Bi·∫øn boolean ƒë·ªÉ check c√≥ d√πng GPU kh√¥ng
    # - D√πng ƒë·ªÉ config pin_memory, persistent_workers...

    # --- 3.5. Create DataLoaders ---
    train_loader = DataLoader(
        train_ds,
        # Dataset ƒë·ªÉ load
        
        batch_size=BATCH_SIZE,
        # Gi·∫£i th√≠ch:
        # - M·ªói l·∫ßn yield 32 ·∫£nh (1 batch)
        # - GPU x·ª≠ l√Ω 32 ·∫£nh song song ‚Üí hi·ªáu qu·∫£
        
        shuffle=True,
        # Gi·∫£i th√≠ch:
        # - X√°o tr·ªôn th·ª© t·ª± ·∫£nh m·ªói epoch
        # - T·∫°i sao? Tr√°nh model h·ªçc "th·ª© t·ª±" thay v√¨ "n·ªôi dung"
        # - V√≠ d·ª•: N·∫øu CR lu√¥n ƒë·∫ßu ti√™n ‚Üí model bias
        
        num_workers=NUM_WORKERS,
        # Gi·∫£i th√≠ch:
        # - S·ªë ti·∫øn tr√¨nh CPU load data song song
        # - 4 workers ‚Üí 4 threads chu·∫©n b·ªã data cho GPU
        # - GPU kh√¥ng ph·∫£i ch·ªù ‚Üí utilization cao
        
        persistent_workers=NUM_WORKERS > 0,
        # Gi·∫£i th√≠ch:
        # - True: Gi·ªØ workers S·ªêNG gi·ªØa c√°c epochs
        # - False: H·ªßy v√† t·∫°o l·∫°i workers m·ªói epoch (ch·∫≠m)
        # - ƒêi·ªÅu ki·ªán: Ch·ªâ b·∫≠t khi c√≥ workers (NUM_WORKERS > 0)
        # - L·ª£i √≠ch: Ti·∫øt ki·ªám 3-5 gi√¢y/epoch
        
        pin_memory=use_cuda,
        # Gi·∫£i th√≠ch:
        # - True: Lock memory v√†o RAM, transfer GPU nhanh h∆°n
        # - C∆° ch·∫ø: Pageable RAM ‚Üí Pinned RAM ‚Üí GPU VRAM
        # - Ch·ªâ b·∫≠t khi c√≥ CUDA v√¨ kh√¥ng c·∫ßn thi·∫øt cho CPU
        # - L·ª£i √≠ch: Transfer nhanh h∆°n 10-20%
    )
                               
    val_loader = DataLoader(
        val_ds,
        batch_size=BATCH_SIZE,
        
        shuffle=False,
        # Gi·∫£i th√≠ch:
        # - Validation KH√îNG shuffle
        # - T·∫°i sao? Kh√¥ng c·∫ßn v√¨ kh√¥ng training
        # - Gi·ªØ th·ª© t·ª± gi√∫p debug d·ªÖ h∆°n (bi·∫øt ·∫£nh n√†o sai)
        
        num_workers=NUM_WORKERS,
        persistent_workers=NUM_WORKERS > 0,
        pin_memory=use_cuda,
        # T∆∞∆°ng t·ª± train_loader
    )

    print(f"‚úÖ ƒê√£ t·∫£i: {len(train_ds)} ·∫£nh Train | {len(val_ds)} ·∫£nh Val")
    # In ra s·ªë l∆∞·ª£ng ƒë·ªÉ check split ƒë√∫ng ch∆∞a

    # ============================================================
    # PH·∫¶N 4: X√ÇY D·ª∞NG MODEL
    # Strategy: Partial Freezing + Discriminative Learning Rates
    # ============================================================
    print("üõ†Ô∏è ƒêang kh·ªüi t·∫°o MobileNetV2...")
    
    model = models.mobilenet_v2(weights='DEFAULT')
    # Gi·∫£i th√≠ch:
    # - Load MobileNetV2 pretrained tr√™n ImageNet
    # - weights='DEFAULT': D√πng weights t·ªët nh·∫•t hi·ªán c√≥
    # - Model ƒë√£ h·ªçc: edges, textures, shapes t·ª´ 1.2M ·∫£nh ImageNet
    # - C·∫•u tr√∫c:
    #   * model.features: 17 blocks (convolutional layers)
    #   * model.classifier: 2 layers (avgpool + linear 1280‚Üí1000)

    # --- 4.1. Freeze Backbone ---
    for param in model.features.parameters():
        param.requires_grad = False
    # Gi·∫£i th√≠ch:
    # - ƒê√≥ng bƒÉng T·∫§T C·∫¢ parameters trong features
    # - param.requires_grad = False: Kh√¥ng t√≠nh gradient, kh√¥ng update
    # - T·∫°i sao? Features ƒë√£ h·ªçc t·ªët t·ª´ ImageNet, kh√¥ng c·∫ßn train l·∫°i
    # - L·ª£i √≠ch:
    #   * Training nhanh h∆°n (√≠t parameters)
    #   * √çt VRAM (√≠t gradient)
    #   * Tr√°nh overfit (gi·ªØ ki·∫øn th·ª©c t·ªïng qu√°t)

    # --- 4.2. Unfreeze Last N Blocks ---
    if UNFREEZE_LAST_N_BLOCKS and UNFREEZE_LAST_N_BLOCKS > 0:
        # Gi·∫£i th√≠ch ƒëi·ªÅu ki·ªán:
        # - UNFREEZE_LAST_N_BLOCKS: Check not None/not 0
        # - > 0: Check l√† s·ªë d∆∞∆°ng
        # - Ch·ªâ ch·∫°y n·∫øu mu·ªën unfreeze (c√≥ th·ªÉ set 0 ƒë·ªÉ full freeze)
        
        last_blocks = list(model.features.children())[-UNFREEZE_LAST_N_BLOCKS:]
        # Gi·∫£i th√≠ch:
        # - model.features.children(): Iterator qua c√°c sub-modules
        # - list(...): Convert th√†nh list
        # - [-UNFREEZE_LAST_N_BLOCKS:]: L·∫•y N blocks cu·ªëi
        # - V√≠ d·ª•: N=2 ‚Üí l·∫•y features[15], features[16]
        
        for block in last_blocks:
            for param in block.parameters():
                param.requires_grad = True
        # Gi·∫£i th√≠ch:
        # - L·∫∑p qua t·ª´ng block ƒë∆∞·ª£c ch·ªçn
        # - Set requires_grad = True: B·∫¨T l·∫°i gradient
        # - Blocks n√†y s·∫Ω ƒë∆∞·ª£c fine-tune v·ªõi BACKBONE_LR
        # - T·∫°i sao blocks cu·ªëi? H·ªçc "high-level features" g·∫ßn task h∆°n

    # --- 4.3. Replace Classifier Head ---
    model.classifier[1] = nn.Sequential(
        nn.Dropout(0.2),
        # Gi·∫£i th√≠ch:
        # - Dropout: T·∫Øt ng·∫´u nhi√™n 20% neurons m·ªói forward pass
        # - C√¥ng th·ª©c: output = input * mask (mask: 80% l√† 1, 20% l√† 0)
        # - T·∫°i sao? Tr√°nh overfitting, model ph·ª• thu·ªôc nhi·ªÅu neurons
        # - Training: Dropout B·∫¨T, Inference: Dropout T·∫ÆT
        
        nn.Linear(model.last_channel, NUM_CLASSES)
        # Gi·∫£i th√≠ch:
        # - Linear: Fully connected layer (y = Wx + b)
        # - model.last_channel: 1280 (output c·ªßa features)
        # - NUM_CLASSES: 4 (CR, LP, OK, PO)
        # - Shape: [batch, 1280] ‚Üí [batch, 4]
        # - Layer n√†y LU√îN requires_grad=True (m·ªõi t·∫°o, ch∆∞a train)
    )
    
    model = model.to(DEVICE)
    # Gi·∫£i th√≠ch:
    # - Chuy·ªÉn to√†n b·ªô model l√™n GPU/CPU
    # - L√†m 1 L·∫¶N ·ªü ƒë√¢y, kh√¥ng trong loop
    # - Sau n√†y ch·ªâ c·∫ßn chuy·ªÉn data: images.to(DEVICE)

    # ============================================================
    # PH·∫¶N 5: C√îNG C·ª§ HU·∫§N LUY·ªÜN
    # Loss + Optimizer + Scheduler + AMP
    # ============================================================
    
    # --- 5.1. Loss Function ---
    criterion = nn.CrossEntropyLoss(label_smoothing=0.05)
    # Gi·∫£i th√≠ch:
    # - CrossEntropyLoss: D√πng cho multi-class classification
    # - C√¥ng th·ª©c: -log(softmax(output)[target_class])
    # - label_smoothing=0.05: L√†m m·ªÅm nh√£n
    #   * Thay v√¨: [0, 0, 1, 0] (one-hot)
    #   * Th√†nh:   [0.0125, 0.0125, 0.95, 0.0125] (smoothed)
    # - L·ª£i √≠ch: Model √≠t "overconfident", t·ªïng qu√°t h∆°n

    # --- 5.2. Optimizer v·ªõi Discriminative Learning Rates ---
    backbone_params = [p for p in model.features.parameters() if p.requires_grad]
    # Gi·∫£i th√≠ch:
    # - L·∫•y T·∫§T C·∫¢ parameters trong features C√ì requires_grad=True
    # - List comprehension: [p for p in ... if condition]
    # - K·∫øt qu·∫£: Ch·ªâ c√≥ parameters c·ªßa 2 blocks cu·ªëi (ƒë√£ unfreeze)
    
    head_params = [p for p in model.classifier.parameters() if p.requires_grad]
    # Gi·∫£i th√≠ch:
    # - L·∫•y parameters c·ªßa classifier
    # - T·∫•t c·∫£ ƒë·ªÅu requires_grad=True v√¨ m·ªõi t·∫°o
    
    optimizer = optim.AdamW(
        # Gi·∫£i th√≠ch AdamW:
        # - Adam: Adaptive Moment Estimation (t·ª± ƒëi·ªÅu ch·ªânh LR)
        # - W: Weight decay ƒë∆∞·ª£c implement ƒê√öNG (kh√°c Adam g·ªëc)
        # - T·ªët h∆°n SGD cho transfer learning
        
        [
            {'params': backbone_params, 'lr': BACKBONE_LR},
            # Gi·∫£i th√≠ch:
            # - Group 1: Backbone parameters
            # - lr: 1e-4 (th·∫•p v√¨ ƒë√£ train s·∫µn)
            # - Update nh·∫π nh√†ng, gi·ªØ ki·∫øn th·ª©c c≈©
            
            {'params': head_params, 'lr': HEAD_LR},
            # Gi·∫£i th√≠ch:
            # - Group 2: Head parameters
            # - lr: 1e-3 (cao h∆°n backbone g·∫•p 10)
            # - Update m·∫°nh v√¨ ch∆∞a train, c·∫ßn h·ªçc nhi·ªÅu
        ],
        weight_decay=WEIGHT_DECAY,
        # Gi·∫£i th√≠ch:
        # - L2 regularization: loss += weight_decay * ||weights||¬≤
        # - 1e-4: Ph·∫°t weights l·ªõn, tr√°nh overfit
        # - Apply cho C·∫¢ 2 groups
    )
    
    # --- 5.3. Learning Rate Scheduler ---
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
    # Gi·∫£i th√≠ch:
    # - Cosine Annealing: LR gi·∫£m theo d·∫°ng h√¨nh sin
    # - Warm Restarts: TƒÉng LR l√™n l·∫°i sau m·ªói chu k·ª≥
    # - T_0=5: Chu k·ª≥ ƒë·∫ßu ti√™n 5 epochs
    # - T_mult=2: M·ªói chu k·ª≥ sau d√†i g·∫•p ƒë√¥i tr∆∞·ªõc ƒë√≥
    # - Chu k·ª≥: [0-5], [5-15], [15-35]
    # - Flow:
    #   Epoch 0: LR = 0.001 (max)
    #   Epoch 2.5: LR = 0.0005 (gi·∫£m)
    #   Epoch 5: LR = 0.0001 (min) ‚Üí RESTART ‚Üí 0.001
    # - L·ª£i √≠ch: Restart gi√∫p "nh·∫£y" ra local minimum, t√¨m solution t·ªët h∆°n
    
    # --- 5.4. Mixed Precision Scaler ---
    scaler = GradScaler(enabled=use_cuda)
    # Gi·∫£i th√≠ch:
    # - GradScaler: Scale gradients cho Mixed Precision Training
    # - enabled=use_cuda: Ch·ªâ b·∫≠t khi c√≥ GPU (CPU kh√¥ng h·ªó tr·ª£ FP16)
    # - V·∫•n ƒë·ªÅ FP16: Gradient qu√° nh·ªè ‚Üí underflow ‚Üí 0
    # - Gi·∫£i ph√°p: Nh√¢n gradient l√™n (scale) ‚Üí backward ‚Üí chia xu·ªëng ‚Üí update
    # - V√≠ d·ª•:
    #   * Gradient th·∫≠t: 0.00001
    #   * Scale l√™n: 0.00001 √ó 65536 = 0.65536
    #   * Backward kh√¥ng b·ªã underflow
    #   * Scale xu·ªëng: 0.65536 / 65536 = 0.00001
    #   * Update weights v·ªõi gi√° tr·ªã ƒë√∫ng

    # --- 5.5. Early Stopping Config ---
    best_acc = 0.0
    # Gi·∫£i th√≠ch:
    # - L∆∞u accuracy t·ªët nh·∫•t t·ª´ tr∆∞·ªõc ƒë·∫øn gi·ªù
    # - Kh·ªüi t·∫°o 0.0, s·∫Ω update khi val_acc > best_acc
    
    patience = 5
    # Gi·∫£i th√≠ch:
    # - S·ªë epochs "ch·ªãu ƒë·ª±ng" khi kh√¥ng ti·∫øn b·ªô
    # - N·∫øu 5 epochs li√™n ti·∫øp kh√¥ng c·∫£i thi·ªán ‚Üí D·ª™NG
    
    bad_epochs = 0
    # Gi·∫£i th√≠ch:
    # - ƒê·∫øm s·ªë epochs li√™n ti·∫øp kh√¥ng ti·∫øn b·ªô
    # - Reset v·ªÅ 0 khi c√≥ c·∫£i thi·ªán
    # - TƒÉng l√™n 1 khi kh√¥ng c·∫£i thi·ªán
    # - D·ª´ng khi bad_epochs >= patience

    # ============================================================
    # PH·∫¶N 6: V√íNG L·∫∂P HU·∫§N LUY·ªÜN CH√çNH
    # Flow: Train ‚Üí Validate ‚Üí Update LR ‚Üí Save if best ‚Üí Early Stop
    # ============================================================
    print("\nüöÄ B·∫ÆT ƒê·∫¶U HU·∫§N LUY·ªÜN (ƒêA LU·ªíNG)...")
    
    for epoch in range(NUM_EPOCHS):
        # Gi·∫£i th√≠ch:
        # - L·∫∑p qua NUM_EPOCHS l·∫ßn (t·ªëi ƒëa 50)
        # - M·ªói epoch: model nh√¨n to√†n b·ªô training set 1 l·∫ßn
        # - C√≥ th·ªÉ d·ª´ng s·ªõm n·∫øu bad_epochs >= patience
        
        start_time = time.time()
        # ƒêo th·ªùi gian ƒë·ªÉ bi·∫øt m·ªói epoch m·∫•t bao l√¢u
        
        # ============================================================
        # PH·∫¶N 6.1: TRAINING PHASE
        # ============================================================
        model.train()
        # Gi·∫£i th√≠ch:
        # - Chuy·ªÉn model sang training mode
        # - ·∫¢nh h∆∞·ªüng:
        #   * Dropout: B·∫¨T (t·∫Øt 20% neurons ng·∫´u nhi√™n)
        #   * BatchNorm: C·∫≠p nh·∫≠t running statistics
        #   * Gradient: ƒê∆∞·ª£c t√≠nh to√°n
        
        # --- Kh·ªüi t·∫°o metrics ---
        train_loss = 0
        # Gi·∫£i th√≠ch:
        # - T·ªïng loss c·ªßa to√†n b·ªô training set
        # - S·∫Ω t√≠nh trung b√¨nh sau: train_loss / train_total
        
        train_correct = 0
        # Gi·∫£i th√≠ch:
        # - S·ªë ·∫£nh d·ª± ƒëo√°n ƒê√öNG
        # - D√πng ƒë·ªÉ t√≠nh accuracy: train_correct / train_total
        
        train_total = 0
        # Gi·∫£i th√≠ch:
        # - T·ªïng s·ªë ·∫£nh ƒë√£ x·ª≠ l√Ω
        # - B·∫±ng len(train_ds) sau khi h·∫øt epoch
        
        # --- Progress Bar ---
        loop = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{NUM_EPOCHS}]", leave=True)
        # Gi·∫£i th√≠ch:
        # - tqdm: T·∫°o progress bar ƒë·∫πp
        # - train_loader: Iterable ƒë·ªÉ loop qua
        # - desc: M√¥ t·∫£ hi·ªÉn th·ªã ·ªü ƒë·∫ßu bar
        # - leave=True: Gi·ªØ l·∫°i bar sau khi xong (xem l·ªãch s·ª≠)
        # - Output: Epoch [1/50]: 100%|‚ñà‚ñà‚ñà‚ñà| 125/125 [00:15<00:00, 8.1it/s]
        
        for images, labels in loop:
            # Gi·∫£i th√≠ch:
            # - Loop qua t·ª´ng batch trong train_loader
            # - images: [batch_size, 3, 224, 224] (32 ·∫£nh RGB)
            # - labels: [batch_size] (32 nh√£n: 0-3 cho CR, LP, OK, PO)
            
            # --- Chuy·ªÉn data l√™n GPU ---
            images = images.to(DEVICE, non_blocking=use_cuda)
            labels = labels.to(DEVICE, non_blocking=use_cuda)
            # Gi·∫£i th√≠ch:
            # - .to(DEVICE): Chuy·ªÉn tensor t·ª´ CPU RAM ‚Üí GPU VRAM
            # - non_blocking=True: Async transfer (CPU ti·∫øp t·ª•c ch·∫°y)
            # - Ch·ªâ b·∫≠t non_blocking khi c√≥ CUDA
            # - Flow: CPU chu·∫©n b·ªã batch k·∫ø ‚Üí GPU x·ª≠ l√Ω batch hi·ªán t·∫°i
            # - L·ª£i √≠ch: Gi·∫£m idle time
            
            # --- X√≥a gradient c≈© ---
            optimizer.zero_grad()
            # Gi·∫£i th√≠ch:
            # - PyTorch G√ÇY D·ªíN gradient m·∫∑c ƒë·ªãnh
            # - Batch 1: grad = [0.5, 0.3]
            # - Batch 2: grad = [0.5, 0.3] + [0.2, 0.1] = [0.7, 0.4] ‚Üê SAI!
            # - Ph·∫£i x√≥a tr∆∞·ªõc m·ªói batch:
            # - Batch 1: grad = [0.5, 0.3]
            # - zero_grad() ‚Üí grad = [0, 0]
            # - Batch 2: grad = [0.2, 0.1] ‚Üê ƒê√öNG!
            
            # --- Forward Pass v·ªõi Mixed Precision ---
            amp_ctx = autocast(device_type='cuda', dtype=torch.float16, enabled=use_cuda) if use_cuda else nullcontext()
            # Gi·∫£i th√≠ch:
            # - T·∫°o context manager cho Mixed Precision
            # - use_cuda=True:
            #   * autocast: T·ª± ƒë·ªông ch·ªçn FP16/FP32 cho t·ª´ng op
            #   * device_type='cuda': Ch·ªâ ƒë·ªãnh GPU
            #   * dtype=torch.float16: Precision m·∫∑c ƒë·ªãnh
            # - use_cuda=False:
            #   * nullcontext(): Context manager "r·ªóng", kh√¥ng l√†m g√¨
            #   * CPU kh√¥ng h·ªó tr·ª£ FP16 ‚Üí d√πng FP32 b√¨nh th∆∞·ªùng
            
            with amp_ctx:
                # Gi·∫£i th√≠ch with statement:
                # - V√†o context: autocast b·∫≠t, c√°c ops b√™n trong d√πng FP16
                # - Ra context: autocast t·∫Øt, ops ngo√†i d√πng FP32
                # - T·ª± ƒë·ªông cleanup khi xong ho·∫∑c c√≥ exception
                
                outputs = model(images)
                # Gi·∫£i th√≠ch:
                # - Forward pass: ƒë∆∞a images qua model
                # - images: [32, 3, 224, 224]
                # - outputs: [32, 4] (32 ·∫£nh, 4 scores cho 4 classes)
                # - V·ªõi AMP:
                #   * Convolutions: FP16 (nhanh)
                #   * Matrix multiply: FP16 (nhanh)
                #   * Softmax, loss: FP32 (ch√≠nh x√°c)
                # - Flow:
                #   features ‚Üí [32, 1280]
                #   classifier ‚Üí [32, 4]
                #   V√≠ d·ª• output: [[2.1, -0.5, 1.3, -1.2], ...] (logits)
                
                loss = criterion(outputs, labels)
                # Gi·∫£i th√≠ch:
                # - T√≠nh loss gi·ªØa predictions v√† ground truth
                # - criterion = CrossEntropyLoss
                # - outputs: [32, 4] logits (ch∆∞a softmax)
                # - labels: [32] indices (0-3)
                # - B√™n trong criterion:
                #   1. Softmax: logits ‚Üí probabilities
                #      [2.1, -0.5, 1.3, -1.2] ‚Üí [0.65, 0.05, 0.25, 0.02]
                #   2. Log: -log(prob[correct_class])
                #      Label=0 (CR) ‚Üí -log(0.65) = 0.43
                #   3. Average: mean(losses)
                # - Label smoothing: L√†m m·ªÅm [0,0,1,0] ‚Üí [0.0125,0.0125,0.95,0.0125]
                # - Output: scalar loss (v√≠ d·ª•: 0.543)
            
            # --- Backward Pass v·ªõi Gradient Scaling ---
            if use_cuda:
                # Gi·∫£i th√≠ch ƒëi·ªÅu ki·ªán:
                # - Ch·ªâ d√πng GradScaler khi c√≥ GPU
                # - CPU: backward b√¨nh th∆∞·ªùng
                
                scaler.scale(loss).backward()
                # Gi·∫£i th√≠ch t·ª´ng b∆∞·ªõc:
                # 1. scaler.scale(loss):
                #    - Nh√¢n loss l√™n: loss * scale_factor (v√≠ d·ª•: √ó 65536)
                #    - Loss g·ªëc: 0.543
                #    - Scaled: 0.543 √ó 65536 = 35585.088
                #    - T·∫°i sao? Tr√°nh gradient qu√° nh·ªè (underflow)
                # 
                # 2. .backward():
                #    - T√≠nh gradient ng∆∞·ª£c t·ª´ loss v·ªÅ t·∫•t c·∫£ weights
                #    - Chain rule: ‚àÇloss/‚àÇw = ‚àÇloss/‚àÇoutput √ó ‚àÇoutput/‚àÇw
                #    - L∆∞u gradient v√†o param.grad c·ªßa m·ªói parameter
                #    - Gradient c≈©ng b·ªã scale l√™n √ó 65536
                #    - V√≠ d·ª•:
                #      * Gradient th·∫≠t: 0.00001
                #      * Scaled gradient: 0.00001 √ó 65536 = 0.65536
                #      * Kh√¥ng b·ªã underflow (th√†nh 0) trong FP16
                
                scaler.step(optimizer)
                # Gi·∫£i th√≠ch:
                # 1. Unscale gradients:
                #    - Chia gradient xu·ªëng: grad / scale_factor
                #    - 0.65536 / 65536 = 0.00001 (gradient th·∫≠t)
                # 
                # 2. Check for inf/nan:
                #    - N·∫øu c√≥: Skip update (gradient explosion)
                #    - Gi·∫£m scale_factor cho l·∫ßn sau
                # 
                # 3. optimizer.step():
                #    - C·∫≠p nh·∫≠t weights: w_new = w_old - lr √ó grad
                #    - AdamW th·ª±c t·∫ø ph·ª©c t·∫°p h∆°n (momentum, adaptive lr...)
                #    - Backbone: lr = 1e-4
                #    - Head: lr = 1e-3
                #    - V√≠ d·ª•:
                #      * w_old = 0.5
                #      * grad = 0.00001
                #      * lr = 0.001
                #      * w_new = 0.5 - 0.001 √ó 0.00001 = 0.49999999
                
                scaler.update()
                # Gi·∫£i th√≠ch:
                # - C·∫≠p nh·∫≠t scale_factor cho l·∫ßn sau
                # - N·∫øu kh√¥ng c√≥ inf/nan nhi·ªÅu l·∫ßn ‚Üí tƒÉng scale_factor
                # - N·∫øu c√≥ inf/nan ‚Üí gi·∫£m scale_factor
                # - Dynamic scaling: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh optimal scale
                # - M·ª•c ƒë√≠ch: Maximize precision, minimize underflow
                
            else:
                # Gi·∫£i th√≠ch branch n√†y:
                # - CPU kh√¥ng h·ªó tr·ª£ FP16
                # - Backward & update b√¨nh th∆∞·ªùng
                
                loss.backward()
                # Gi·∫£i th√≠ch:
                # - T√≠nh gradient nh∆∞ tr√™n nh∆∞ng kh√¥ng scale
                # - FP32 c√≥ range l·ªõn ‚Üí kh√¥ng c·∫ßn scale
                
                optimizer.step()
                # Gi·∫£i th√≠ch:
                # - Update weights tr·ª±c ti·∫øp
                # - Kh√¥ng c·∫ßn unscale v√¨ kh√¥ng scale
            
            # --- T√≠nh Metrics cho Batch n√†y ---
            batch_loss = loss.item()
            # Gi·∫£i th√≠ch:
            # - .item(): Chuy·ªÉn tensor scalar ‚Üí Python number
            # - Tensor: tensor(0.543, device='cuda:0') ‚Üí 0.543
            # - T·∫°i sao? T√≠nh to√°n metrics tr√™n CPU, ti·∫øt ki·ªám VRAM
            
            train_loss += batch_loss * images.size(0)
            # Gi·∫£i th√≠ch:
            # - C·ªông d·ªìn loss (c√≥ tr·ªçng s·ªë batch size)
            # - batch_loss: Loss trung b√¨nh c·ªßa 32 ·∫£nh
            # - images.size(0): 32 (batch size)
            # - T·∫°i sao nh√¢n? ƒê·ªÉ t√≠nh weighted average sau
            # - V√≠ d·ª•:
            #   * Batch 1: 32 ·∫£nh, loss=0.5 ‚Üí +16
            #   * Batch 2: 32 ·∫£nh, loss=0.6 ‚Üí +19.2
            #   * ...
            #   * Batch cu·ªëi: 16 ·∫£nh, loss=0.4 ‚Üí +6.4
            #   * Total: 41.6
            #   * Average: 41.6 / 800 (t·ªïng ·∫£nh) = 0.052
            
            _, predicted = torch.max(outputs, 1)
            # Gi·∫£i th√≠ch:
            # - torch.max(outputs, 1): T√¨m max theo dimension 1 (classes)
            # - outputs: [32, 4]
            # - Tr·∫£ v·ªÅ: (values, indices)
            # - V√≠ d·ª•:
            #   output[0] = [2.1, -0.5, 1.3, -1.2]
            #   max value = 2.1, index = 0
            #   predicted[0] = 0 (d·ª± ƒëo√°n class CR)
            # - _: B·ªè qua values, ch·ªâ l·∫•y indices
            # - predicted: [32] tensor ch·ª©a class d·ª± ƒëo√°n (0-3)
            
            train_total += labels.size(0)
            # Gi·∫£i th√≠ch:
            # - ƒê·∫øm t·ªïng s·ªë ·∫£nh ƒë√£ x·ª≠ l√Ω
            # - labels.size(0): 32 (batch size)
            # - Sau epoch: train_total = 800 (s·ªë ·∫£nh training)
            
            train_correct += (predicted == labels).sum().item()
            # Gi·∫£i th√≠ch t·ª´ng ph·∫ßn:
            # 1. (predicted == labels):
            #    - So s√°nh element-wise
            #    - predicted: [0, 2, 1, 3, ...]
            #    - labels:    [0, 1, 1, 3, ...]
            #    - Result:    [True, False, True, True, ...]
            # 
            # 2. .sum():
            #    - ƒê·∫øm s·ªë True
            #    - True = 1, False = 0
            #    - V√≠ d·ª•: [True, False, True, True] ‚Üí 3
            # 
            # 3. .item():
            #    - Chuy·ªÉn tensor ‚Üí Python int
            #    - tensor(3) ‚Üí 3
            # 
            # - K·∫øt qu·∫£: S·ªë ·∫£nh d·ª± ƒëo√°n ƒë√∫ng trong batch n√†y
            
            loop.set_postfix(loss=batch_loss, acc=train_correct/train_total)
            # Gi·∫£i th√≠ch:
            # - C·∫≠p nh·∫≠t th√¥ng tin hi·ªÉn th·ªã ·ªü cu·ªëi progress bar
            # - loss: Loss c·ªßa batch hi·ªán t·∫°i
            # - acc: Accuracy t√≠ch l≈©y t·ª´ ƒë·∫ßu epoch
            # - Output: [...loss=0.543, acc=0.78]
            # - Real-time monitoring: Nh√¨n ngay bi·∫øt training c√≥ ·ªïn kh√¥ng
            
        # --- T√≠nh Metrics T·ªïng cho To√†n B·ªô Training Set ---
        train_acc = train_correct / train_total
        # Gi·∫£i th√≠ch:
        # - Accuracy = S·ªë ƒë√∫ng / T·ªïng s·ªë
        # - V√≠ d·ª•: 650 / 800 = 0.8125 = 81.25%
        # - ƒê√¢y l√† accuracy tr√™n TRAINING set
        
        train_loss_avg = train_loss / train_total
        # Gi·∫£i th√≠ch:
        # - Loss trung b√¨nh = T·ªïng loss / T·ªïng s·ªë ·∫£nh
        # - Weighted average v√¨ batch cu·ªëi c√≥ th·ªÉ nh·ªè h∆°n
        # - V√≠ d·ª•: 41.6 / 800 = 0.052
        
        # ============================================================
        # PH·∫¶N 6.2: VALIDATION PHASE
        # ============================================================
        model.eval()
        # Gi·∫£i th√≠ch:
        # - Chuy·ªÉn sang evaluation mode
        # - ·∫¢nh h∆∞·ªüng:
        #   * Dropout: T·∫ÆT (d√πng 100% neurons)
        #   * BatchNorm: D√πng running stats ƒë√£ l∆∞u (kh√¥ng update)
        #   * Gradient: Kh√¥ng t√≠nh (trong torch.no_grad())
        # - T·∫°i sao c·∫ßn? Mu·ªën ƒë√°nh gi√° ƒê√öNG kh·∫£ nƒÉng c·ªßa model
        
        # --- Kh·ªüi t·∫°o metrics ---
        val_loss = 0
        val_correct = 0
        val_total = 0
        # T∆∞∆°ng t·ª± train metrics
        
        with torch.no_grad():
            # Gi·∫£i th√≠ch:
            # - T·∫Øt gradient computation
            # - T·∫°i sao?
            #   * Validation kh√¥ng c·∫ßn gradient (kh√¥ng update weights)
            #   * Ti·∫øt ki·ªám memory: Kh√¥ng l∆∞u computation graph
            #   * Nhanh h∆°n: Kh√¥ng t√≠nh to√°n gradient
            # - L·ª£i √≠ch:
            #   * Memory gi·∫£m 50%
            #   * T·ªëc ƒë·ªô tƒÉng 30%
            # - C∆° ch·∫ø:
            #   * Tensor.requires_grad = False temporarily
            #   * Kh√¥ng build computation graph
            
            for images, labels in val_loader:
                # Gi·∫£i th√≠ch:
                # - Loop qua validation set
                # - Kh√¥ng c√≥ progress bar (val nhanh h∆°n train)
                # - Kh√¥ng shuffle (th·ª© t·ª± c·ªë ƒë·ªãnh)
                
                images = images.to(DEVICE, non_blocking=use_cuda)
                labels = labels.to(DEVICE, non_blocking=use_cuda)
                # Chuy·ªÉn data l√™n GPU, t∆∞∆°ng t·ª± train
                
                outputs = model(images)
                # Gi·∫£i th√≠ch:
                # - Forward pass KH√îNG c√≥ autocast
                # - T·∫°i sao? ƒê√£ ·ªü ngo√†i training loop, d√πng FP32 ƒë·∫ßy ƒë·ªß
                # - Val c·∫ßn ch√≠nh x√°c tuy·ªát ƒë·ªëi ‚Üí FP32 t·ªët h∆°n
                # - outputs: [32, 4] logits
                
                loss = criterion(outputs, labels)
                # T√≠nh loss, t∆∞∆°ng t·ª± train
                
                val_loss += loss.item() * images.size(0)
                # C·ªông d·ªìn loss (weighted)
                
                _, predicted = torch.max(outputs, 1)
                # L·∫•y class prediction
                
                val_total += labels.size(0)
                # ƒê·∫øm t·ªïng s·ªë ·∫£nh
                
                val_correct += (predicted == labels).sum().item()
                # ƒê·∫øm s·ªë d·ª± ƒëo√°n ƒë√∫ng
                
        # --- T√≠nh Metrics Validation ---
        val_acc = val_correct / val_total
        # Gi·∫£i th√≠ch:
        # - Accuracy tr√™n validation set
        # - ƒê√ÇY L√Ä METRIC QUAN TR·ªåNG NH·∫§T
        # - ƒê√°nh gi√° kh·∫£ nƒÉng t·ªïng qu√°t c·ªßa model
        # - So s√°nh:
        #   * train_acc = 90%, val_acc = 85% ‚Üí OK (generalize t·ªët)
        #   * train_acc = 95%, val_acc = 70% ‚Üí OVERFIT (h·ªçc thu·ªôc)
        #   * train_acc = 60%, val_acc = 58% ‚Üí UNDERFIT (ch∆∞a h·ªçc ƒë·ªß)
        
        val_loss_avg = val_loss / val_total
        # Loss trung b√¨nh validation
        
        # ============================================================
        # PH·∫¶N 6.3: UPDATE LEARNING RATE
        # ============================================================
        scheduler.step(epoch)
        # Gi·∫£i th√≠ch:
        # - C·∫≠p nh·∫≠t learning rate theo scheduler
        # - CosineAnnealingWarmRestarts:
        #   * Input: epoch number
        #   * Output: ƒêi·ªÅu ch·ªânh optimizer.param_groups[i]['lr']
        # - Flow:
        #   Epoch 0: lr_backbone=1e-4, lr_head=1e-3
        #   Epoch 2: lr gi·∫£m theo cosine
        #   Epoch 5: RESTART ‚Üí lr_backbone=1e-4, lr_head=1e-3
        # - T·ª± ƒë·ªông, kh√¥ng c·∫ßn l√†m g√¨ th√™m
        
        # ============================================================
        # PH·∫¶N 6.4: LOGGING & DISPLAY
        # ============================================================
        print(f"üëâ KQ: Train Acc: {train_acc:.1%} | Val Acc: {val_acc:.1%} (Loss: {val_loss_avg:.4f})")
        # Gi·∫£i th√≠ch format strings:
        # - {train_acc:.1%}: Format percentage, 1 s·ªë th·∫≠p ph√¢n
        #   * 0.8125 ‚Üí 81.2%
        # - {val_loss_avg:.4f}: Format float, 4 s·ªë th·∫≠p ph√¢n
        #   * 0.052134 ‚Üí 0.0521
        # - Output: üëâ KQ: Train Acc: 81.2% | Val Acc: 78.5% (Loss: 0.0521)
        
        # ============================================================
        # PH·∫¶N 6.5: SAVE BEST MODEL
        # ============================================================
        if val_acc > best_acc:
            # Gi·∫£i th√≠ch ƒëi·ªÅu ki·ªán:
            # - Ch·ªâ save khi val_acc T·ªêT H∆†N best_acc
            # - V√≠ d·ª•:
            #   * Epoch 1: val_acc=0.75, best_acc=0 ‚Üí Save, best_acc=0.75
            #   * Epoch 2: val_acc=0.73, best_acc=0.75 ‚Üí Kh√¥ng save
            #   * Epoch 3: val_acc=0.78, best_acc=0.75 ‚Üí Save, best_acc=0.78
            
            best_acc = val_acc
            # Gi·∫£i th√≠ch:
            # - Update best_acc v·ªõi gi√° tr·ªã m·ªõi
            # - D√πng ƒë·ªÉ so s√°nh c√°c epochs sau
            
            torch.save(model.state_dict(), 'best_mobilenet_hybrid.pth')
            # Gi·∫£i th√≠ch:
            # - L∆∞u model weights v√†o file
            # - model.state_dict(): Dictionary ch·ª©a t·∫•t c·∫£ parameters
            #   {
            #     'features.0.0.weight': tensor([...]),
            #     'features.0.0.bias': tensor([...]),
            #     ...
            #     'classifier.1.0.weight': tensor([...]),
            #   }
            # - 'best_mobilenet_hybrid.pth': T√™n file
            # - .pth: Extension chu·∫©n cho PyTorch
            # - Ch·ªâ l∆∞u weights, KH√îNG l∆∞u:
            #   * Ki·∫øn tr√∫c model (ph·∫£i define l·∫°i khi load)
            #   * Optimizer state
            #   * Training history
            # - K√≠ch th∆∞·ªõc file: ~14MB (MobileNetV2)
            
            print("üíæ ƒê√£ l∆∞u K·ª∂ L·ª§C M·ªöI!")
            # Th√¥ng b√°o cho user bi·∫øt
            
            bad_epochs = 0
            # Gi·∫£i th√≠ch:
            # - Reset counter v·ªÅ 0 v√¨ c√≥ ti·∫øn b·ªô
            # - B·∫Øt ƒë·∫ßu ƒë·∫øm l·∫°i t·ª´ ƒë·∫ßu
            
        else:
            # Gi·∫£i th√≠ch else:
            # - Tr∆∞·ªùng h·ª£p val_acc KH√îNG t·ªët h∆°n best_acc
            # - Model kh√¥ng c·∫£i thi·ªán
            
            bad_epochs += 1
            # Gi·∫£i th√≠ch:
            # - TƒÉng counter l√™n 1
            # - ƒê√°nh d·∫•u 1 epoch "th·∫•t b·∫°i"
            # - V√≠ d·ª•:
            #   * Epoch 10: val_acc gi·∫£m ‚Üí bad_epochs = 1
            #   * Epoch 11: val_acc gi·∫£m ‚Üí bad_epochs = 2
            #   * Epoch 12: val_acc gi·∫£m ‚Üí bad_epochs = 3
            #   * Epoch 13: val_acc tƒÉng ‚Üí bad_epochs = 0 (reset)
            
        # ============================================================
        # PH·∫¶N 6.6: EARLY STOPPING CHECK
        # ============================================================
        if bad_epochs >= patience:
            # Gi·∫£i th√≠ch ƒëi·ªÅu ki·ªán:
            # - patience = 5: Ch·ªãu ƒë·ª±ng t·ªëi ƒëa 5 epochs kh√¥ng ti·∫øn b·ªô
            # - bad_epochs >= 5: ƒê√£ 5 epochs li√™n ti·∫øp kh√¥ng c·∫£i thi·ªán
            # - K·∫øt lu·∫≠n: Model ƒë√£ h·ªôi t·ª•, ti·∫øp t·ª•c train = l√£ng ph√≠ th·ªùi gian
            
            print(f"‚õî D·ª™NG S·ªöM!")
            # Th√¥ng b√°o d·ª´ng s·ªõm
            
            break
            # Gi·∫£i th√≠ch:
            # - Tho√°t kh·ªèi v√≤ng for epoch
            # - Kh√¥ng ch·∫°y c√°c epochs c√≤n l·∫°i
            # - V√≠ d·ª•:
            #   * NUM_EPOCHS = 50
            #   * Epoch 18: bad_epochs = 5
            #   * Break ‚Üí D·ª´ng ·ªü epoch 18, kh√¥ng ch·∫°y 19-50
            # - L·ª£i √≠ch:
            #   * Ti·∫øt ki·ªám th·ªùi gian (32 epochs √ó 30s = 16 ph√∫t)
            #   * Tr√°nh overfit (train th√™m kh√¥ng gi√∫p g√¨)
            #   * T·ª± ƒë·ªông: Kh√¥ng c·∫ßn babysit

    # ============================================================
    # PH·∫¶N 7: K·∫æT TH√öC TRAINING
    # ============================================================
    print(f"üèÅ XONG! Best Acc: {best_acc:.1%}")
    # Gi·∫£i th√≠ch:
    # - In ra accuracy t·ªët nh·∫•t ƒë·∫°t ƒë∆∞·ª£c
    # - best_acc: Gi√° tr·ªã cao nh·∫•t trong qu√° tr√¨nh training
    # - V√≠ d·ª•: üèÅ XONG! Best Acc: 87.3%
    # - ƒê√¢y l√† k·∫øt qu·∫£ cu·ªëi c√πng c·ªßa model

# ============================================================
# PH·∫¶N 8: S·ª¨ D·ª§NG MODEL ƒê√É TRAIN
# ============================================================
"""
Sau khi train xong, load model ƒë·ªÉ inference:

# Load model
model = models.mobilenet_v2()
model.classifier[1] = nn.Sequential(
    nn.Dropout(0.2),
    nn.Linear(1280, 4)
)
model.load_state_dict(torch.load('best_mobilenet_hybrid.pth'))
model.eval()
model = model.to('cuda')

# Predict 1 ·∫£nh
from PIL import Image
import torchvision.transforms as transforms

img = Image.open('test.jpg')
transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
])
img_tensor = transform(img).unsqueeze(0).to('cuda')

with torch.no_grad():
    output = model(img_tensor)
    _, predicted = torch.max(output, 1)
    class_names = ['CR', 'LP', 'OK', 'PO']
    print(f"Predicted: {class_names[predicted.item()]}")
"""

# ============================================================
# T√ìM T·∫ÆT FLOW T·ªîNG TH·ªÇ
# ============================================================
"""
1. SETUP:
   - T·∫°o random seed ‚Üí Reproducible
   - Load data ‚Üí Split 80/20
   - Separate transforms cho train/val
   
2. MODEL:
   - Load pretrained MobileNetV2
   - Freeze backbone (features)
   - Unfreeze 2 blocks cu·ªëi
   - Replace classifier (1000 ‚Üí 4 classes)
   
3. TRAINING TOOLS:
   - Loss: CrossEntropyLoss + label smoothing
   - Optimizer: AdamW v·ªõi 2 learning rates
   - Scheduler: CosineAnnealingWarmRestarts
   - AMP: GradScaler cho mixed precision
   
4. TRAINING LOOP (m·ªói epoch):
   a. TRAIN:
      - model.train()
      - Loop qua train_loader
      - Forward v·ªõi AMP
      - Backward v·ªõi gradient scaling
      - Update weights
      - T√≠nh accuracy
   
   b. VALIDATION:
      - model.eval()
      - torch.no_grad()
      - Forward (FP32)
      - T√≠nh accuracy
   
   c. UPDATE & SAVE:
      - Scheduler.step()
      - If val_acc > best_acc: Save model
      - Else: bad_epochs += 1
   
   d. EARLY STOP:
      - If bad_epochs >= patience: Break
   
5. RESULT:
   - Best model saved t·∫°i 'best_mobilenet_hybrid.pth'
   - Best accuracy: ~85-90%
   - Training time: ~5-10 ph√∫t (20 epochs)
"""