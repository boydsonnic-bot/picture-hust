import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
from torchvision import models
import os
import time
import random
from tqdm import tqdm
from torch.cuda.amp import autocast, GradScaler
import sys
sys.path.append('..')
from hybrid.data import Cv2PreprocessDataset, transform_config

# ============================================================
# CODE CÅ¨ - train.py version trÆ°á»›c khi chá»‰nh unfreeze + param groups
# Best Acc Ä‘áº¡t ~62.5%
# ============================================================

# ============================================================
# 1. Cáº¤U HÃŒNH (Äá»‚ NGOÃ€I Äá»‚ GLOBAL DÃ™NG ÄÆ¯á»¢C)
# ============================================================
DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'
# Cáº¥u hÃ¬nh nháº¹ cho CPU/mÃ¡y yáº¿u; tÄƒng dáº§n náº¿u Ä‘á»§ RAM/GPU
BATCH_SIZE = 32
NUM_WORKERS = 4  # Ä‘áº·t 0 cho CPU yáº¿u; tÄƒng 1-2 náº¿u cÃ²n dÆ° RAM
NUM_EPOCHS = 20
LR = 5e-3
NUM_CLASSES = 4
DATA_PATH = r'C:\project\picture-hust\data\train'

# ============================================================
# QUAN TRá»ŒNG: CÃ‚U Lá»†NH IF "THáº¦N THÃNH"
# Má»i logic cháº¡y code pháº£i náº±m sau dÃ²ng nÃ y
# ============================================================
if __name__ == '__main__':
    # Äáº·t seed Ä‘á»ƒ tÃ¡i láº­p
    SEED = 42
    random.seed(SEED)
    torch.manual_seed(SEED)
    torch.cuda.manual_seed_all(SEED)

    print(f"ğŸ”¥ Hardware: {DEVICE} | Workers: {NUM_WORKERS}")
    
    # ============================================================
    # 2. CHUáº¨N Bá»Š Dá»® LIá»†U
    # ============================================================
    print("ğŸ“‚ Äang Ä‘á»c dá»¯ liá»‡u...")
    # DÃ¹ng transform riÃªng cho train/val Ä‘á»ƒ trÃ¡nh augment vÃ o val
    full_ds = Cv2PreprocessDataset(DATA_PATH, transform=None)
    train_size = int(0.8 * len(full_ds))
    val_size = len(full_ds) - train_size
    train_indices, val_indices = random_split(range(len(full_ds)), [train_size, val_size])

    # Táº¡o dataset train/val riÃªng Ä‘á»ƒ gÃ¡n transform khÃ¡c nhau
    train_ds = Cv2PreprocessDataset(DATA_PATH, transform=transform_config['train'])
    val_ds   = Cv2PreprocessDataset(DATA_PATH, transform=transform_config['val'])
    train_ds.samples = [full_ds.samples[i] for i in train_indices]
    val_ds.samples   = [full_ds.samples[i] for i in val_indices]

    # persistent_workers dÃ¹ng khi num_workers > 0
    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,
                              num_workers=NUM_WORKERS, persistent_workers=NUM_WORKERS>0)
                               
    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False,
                            num_workers=NUM_WORKERS, persistent_workers=NUM_WORKERS>0)

    print(f"âœ… ÄÃ£ táº£i: {len(train_ds)} áº£nh Train | {len(val_ds)} áº£nh Val")

    # ============================================================
    # 3. XÃ‚Y Dá»°NG MODEL
    # ============================================================
    print("ğŸ› ï¸ Äang khá»Ÿi táº¡o MobileNetV2...")
    model = models.mobilenet_v2(weights='DEFAULT')

    # Freeze backbone, chá»‰ fine-tune classifier cho nhanh/há»c dá»…
    for param in model.features.parameters():
        param.requires_grad = False

    model.classifier[1] = nn.Sequential(
        nn.Dropout(0.2),
        nn.Linear(model.last_channel, NUM_CLASSES)
    )
    model = model.to(DEVICE)

    # ============================================================
    # 4. CÃ”NG Cá»¤ HUáº¤N LUYá»†N
    # ============================================================
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=LR)
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=5, T_mult=2)
    scaler = GradScaler()

    best_acc = 0.0
    patience = 5
    bad_epochs = 0

    # ============================================================
    # 5. VÃ’NG Láº¶P HUáº¤N LUYá»†N
    # ============================================================
    print("\nğŸš€ Báº®T Äáº¦U HUáº¤N LUYá»†N (ÄA LUá»’NG)...")
    
    for epoch in range(NUM_EPOCHS):
        start_time = time.time()
        
        # --- TRAIN ---
        model.train()
        train_loss = 0
        train_correct = 0
        train_total = 0
        
        loop = tqdm(train_loader, desc=f"Epoch [{epoch+1}/{NUM_EPOCHS}]", leave=True)
        
        for images, labels in loop:
            images, labels = images.to(DEVICE), labels.to(DEVICE)
            
            optimizer.zero_grad()
            
            with autocast():
                outputs = model(images)
                loss = criterion(outputs, labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            batch_loss = loss.item()
            train_loss += batch_loss * images.size(0)
            _, predicted = torch.max(outputs, 1)
            train_total += labels.size(0)
            train_correct += (predicted == labels).sum().item()
            
            loop.set_postfix(loss=batch_loss, acc=train_correct/train_total)
            
        train_acc = train_correct / train_total
        train_loss_avg = train_loss / train_total
        
        # --- VAL ---
        model.eval()
        val_loss = 0
        val_correct = 0
        val_total = 0
        
        with torch.no_grad():
            # LÆ°u Ã½: Val loader cÅ©ng dÃ¹ng worker nÃªn sáº½ nhanh hÆ¡n
            for images, labels in val_loader:
                images, labels = images.to(DEVICE), labels.to(DEVICE)
                outputs = model(images)
                loss = criterion(outputs, labels)
                
                val_loss += loss.item() * images.size(0)
                _, predicted = torch.max(outputs, 1)
                val_total += labels.size(0)
                val_correct += (predicted == labels).sum().item()
                
        val_acc = val_correct / val_total
        val_loss_avg = val_loss / val_total
        
        scheduler.step(epoch)
        
        print(f"ğŸ‘‰ KQ: Train Acc: {train_acc:.1%} | Val Acc: {val_acc:.1%} (Loss: {val_loss_avg:.4f})")
        
        if val_acc > best_acc:
            best_acc = val_acc
            torch.save(model.state_dict(), 'best_mobilenet_hybrid.pth')
            print("ğŸ’¾ ÄÃ£ lÆ°u Ká»¶ Lá»¤C Má»šI!")
            bad_epochs = 0
        else:
            bad_epochs += 1
            
        if bad_epochs >= patience:
            print(f"â›” Dá»ªNG Sá»šM!")
            break

    print(f"ğŸ XONG! Best Acc: {best_acc:.1%}")
