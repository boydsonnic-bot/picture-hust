# Lá»™ trÃ¬nh há»c Computer Vision (tá»« dá»… â†’ khÃ³)

**Má»¥c tiÃªu**: NÃ¢ng cáº¥p tá»« code phÃ¡t hiá»‡n contour cÆ¡ báº£n (`test02.py`) lÃªn há»‡ thá»‘ng phÃ¡t hiá»‡n khuyáº¿t táº­t tá»± Ä‘á»™ng (detection/classification) vá»›i Deep Learning.

**NguyÃªn táº¯c**: Kiáº¿n thá»©c tá»« dá»… â†’ khÃ³; há»c tá»«ng giai Ä‘oáº¡n, vá»«a Ä‘á»c lÃ½ thuyáº¿t vá»«a code thá»±c hÃ nh.

---

## ğŸ“š PhÃ¢n tÃ­ch code hiá»‡n táº¡i (`test02.py`)

**Code báº¡n Ä‘ang cÃ³**:
```python
gray â†’ GaussianBlur â†’ Otsu threshold â†’ findContours â†’ boundingRect â†’ save
```

**Äiá»ƒm máº¡nh**: 
- Xá»­ lÃ½ áº£nh cÆ¡ báº£n (grayscale, blur, threshold)
- PhÃ¡t hiá»‡n contour vÃ  tÃ­nh area
- CLI arguments, save káº¿t quáº£

**Háº¡n cháº¿ (cáº§n nÃ¢ng cáº¥p)**:
- KhÃ´ng cÃ³ tiá»n xá»­ lÃ½ nÃ¢ng cao (CLAHE, morphology, adaptive threshold)
- ChÆ°a phÃ¢n loáº¡i (classify) contour lÃ  khuyáº¿t táº­t hay nhiá»…u
- ChÆ°a dÃ¹ng Deep Learning (CNN) Ä‘á»ƒ há»c feature tá»± Ä‘á»™ng
- ChÆ°a cÃ³ detection model (YOLO, Faster R-CNN) Ä‘á»ƒ Ä‘á»‹nh vá»‹ chÃ­nh xÃ¡c

---

## ğŸ—“ï¸ Lá»™ trÃ¬nh há»c (Easy â†’ Hard)

### **Giai Ä‘oáº¡n 1: Classical Computer Vision (Ná»n táº£ng xá»­ lÃ½ áº£nh)**

**Má»¥c tiÃªu**: NÃ¢ng cáº¥p preprocessing pipeline (tiá»n xá»­ lÃ½ áº£nh tá»‘t hÆ¡n)

#### Pháº§n 1: Adaptive Thresholding & CLAHE
- **Äá»c tá»« PDF** (ChÆ°Æ¡ng 1-2 hoáº·c pháº§n cÆ¡ báº£n): Python basics, NumPy, Matplotlib
- **Äá»c thÃªm (Google search)**:
  - `CLAHE OpenCV` (Contrast Limited Adaptive Histogram Equalization)
  - `Adaptive Threshold vs Otsu`
  - `Morphological operations erosion dilation`
- **Key concepts**:
  - **CLAHE**: TÄƒng contrast cá»¥c bá»™ (tá»‘t cho áº£nh X-ray cÃ³ Ä‘á»™ sÃ¡ng khÃ´ng Ä‘á»u)
  - **Adaptive Threshold**: Threshold Ä‘á»™ng theo vÃ¹ng (tá»‘t hÆ¡n Otsu khi áº£nh cÃ³ lighting khÃ´ng Ä‘á»“ng nháº¥t)
  - **Morphology (erosion/dilation/opening/closing)**: Loáº¡i bá» noise, lÃ m má»‹n contour

- **Code nÃ¢ng cáº¥p (3-4h)**:
  ```python
  # test02_v2.py - add CLAHE + adaptive threshold
  clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
  enhanced = clahe.apply(gray)
  adaptive_thresh = cv2.adaptiveThreshold(enhanced, 255, 
                                           cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
                                           cv2.THRESH_BINARY_INV, 11, 2)
  # Morphology
  kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (3,3))
  morph = cv2.morphologyEx(adaptive_thresh, cv2.MORPH_CLOSE, kernel)
  ```

- **Output**: So sÃ¡nh Otsu vs Adaptive + CLAHE side-by-side

#### Pháº§n 2: Feature Engineering (Geometric features)
- **Äá»c thÃªm**:
  - `Contour features OpenCV` (area, perimeter, circularity, aspect ratio)
  - `Hu Moments invariant features`
- **Key concepts**:
  - **Geometric features**: Area, perimeter, circularity = 4Ï€Ã—area/perimeterÂ², aspect ratio = w/h
  - **Hu Moments**: Báº¥t biáº¿n vá»›i rotation, scale (dÃ¹ng Ä‘á»ƒ mÃ´ táº£ hÃ¬nh dáº¡ng)

- **Code nÃ¢ng cáº¥p (3-4h)**:
  ```python
  def extract_features(contour):
      area = cv2.contourArea(contour)
      perimeter = cv2.arcLength(contour, True)
      circularity = 4 * np.pi * area / (perimeter**2) if perimeter > 0 else 0
      x,y,w,h = cv2.boundingRect(contour)
      aspect_ratio = w / h if h > 0 else 0
      hu_moments = cv2.HuMoments(cv2.moments(contour)).flatten()
      return [area, perimeter, circularity, aspect_ratio] + list(hu_moments)
  ```

- **Output**: CSV file chá»©a features cá»§a má»—i contour, filter contour theo rule-based (vÃ­ dá»¥: circularity < 0.5 â†’ cÃ³ thá»ƒ lÃ  khuyáº¿t táº­t dáº¡ng crack)

**ğŸ“Œ Keywords giai Ä‘oáº¡n 1**: `CLAHE`, `Adaptive Threshold`, `Morphology`, `Contour Features`, `Hu Moments`

---

### **Giai Ä‘oáº¡n 2: Deep Learning Basics (CNN cÆ¡ báº£n cho Classification)**

**Má»¥c tiÃªu**: Há»c CNN Ä‘á»ƒ phÃ¢n loáº¡i áº£nh (OK vs NG) hoáº·c phÃ¢n loáº¡i tá»«ng contour

#### Pháº§n 1: CNN Architecture & Transfer Learning
- **Äá»c tá»« PDF**:
  - **ChÆ°Æ¡ng 3: Linear Regression** (trang 49-59) â†’ hiá»ƒu Loss function (MSE, MAE), Gradient Descent, Regularization
  - **ChÆ°Æ¡ng 4-6: Neural Network basics, Backpropagation, CNN** (náº¿u cÃ³) â†’ hiá»ƒu Convolution, Pooling, Activation (ReLU)
  
- **Äá»c thÃªm**:
  - `CNN explained simple` (3Blue1Brown YouTube hoáº·c blog)
  - `Transfer Learning PyTorch/TensorFlow`
  - `ResNet MobileNet architecture`

- **Key concepts**:
  - **Convolution**: Kernel/filter trÃ­ch xuáº¥t feature tá»« áº£nh
    - Formula: `output_size = (input - kernel + 2Ã—padding) / stride + 1`
    - Receptive field: VÃ¹ng áº£nh mÃ  má»—i neuron "nhÃ¬n tháº¥y"
  - **Pooling**: MaxPooling/AvgPooling giáº£m kÃ­ch thÆ°á»›c spatial
  - **Transfer Learning**: DÃ¹ng pretrained model (ResNet, MobileNet) â†’ fine-tune trÃªn dataset nhá» cá»§a báº¡n
  - **Loss**: CrossEntropyLoss (classification), Binary CrossEntropy (binary classification)

- **Code (4-5h)**:
  ```python
  # classifier_v1.py - Binary classification (OK vs Defect)
  import torch
  import torchvision.models as models
  
  model = models.resnet18(pretrained=True)
  model.fc = torch.nn.Linear(model.fc.in_features, 2)  # 2 classes: OK, NG
  
  # Training loop (simplified)
  criterion = torch.nn.CrossEntropyLoss()
  optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
  ```

- **Output**: Model phÃ¢n loáº¡i áº£nh toÃ n bá»™ (full image) thÃ nh OK/NG vá»›i accuracy ~80-90%

#### Pháº§n 2: Data Augmentation & Training
- **Äá»c tá»« PDF**:
  - **ChÆ°Æ¡ng Regularization** (trang 49-59): Dropout, L2 weight decay, BatchNorm, Early Stopping
  - **ChÆ°Æ¡ng Training & Optimization** (trang 27-35): Learning rate scheduling, Adam vs SGD

- **Äá»c thÃªm**:
  - `Data Augmentation for small dataset`
  - `imgaug albumentations library`
  - `Learning rate scheduler PyTorch`

- **Key concepts**:
  - **Augmentation**: Rotation (Â±10Â°), Horizontal flip, Brightness/Contrast, Noise (cáº©n tháº­n vá»›i vertical flip cho áº£nh X-ray)
  - **Regularization**: 
    - Dropout (0.3-0.5) á»Ÿ fully-connected layers
    - L2 weight decay (1e-4)
    - BatchNorm (sau Conv, trÆ°á»›c ReLU)
  - **LR Scheduler**: ReduceLROnPlateau (giáº£m LR khi val_loss khÃ´ng cáº£i thiá»‡n), CosineAnnealing

- **Code (4-5h)**:
  ```python
  import albumentations as A
  
  transform = A.Compose([
      A.Rotate(limit=15, p=0.5),
      A.HorizontalFlip(p=0.5),
      A.RandomBrightnessContrast(p=0.3),
      A.GaussNoise(p=0.2)
  ])
  
  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', 
                                                           factor=0.5, patience=5)
  ```

- **Output**: Model vá»›i augmentation + regularization, accuracy cáº£i thiá»‡n ~5-10%, training curve (loss/accuracy plot)

**ğŸ“Œ Keywords giai Ä‘oáº¡n 2**: `CNN`, `Convolution`, `Pooling`, `Transfer Learning`, `ResNet`, `MobileNet`, `Data Augmentation`, `Dropout`, `Learning Rate Scheduler`

---

### **Giai Ä‘oáº¡n 3: Object Detection (PhÃ¡t hiá»‡n & Äá»‹nh vá»‹ khuyáº¿t táº­t)**

**Má»¥c tiÃªu**: DÃ¹ng YOLO hoáº·c Faster R-CNN Ä‘á»ƒ phÃ¡t hiá»‡n vá»‹ trÃ­ khuyáº¿t táº­t (bounding box)

#### Pháº§n 1: YOLO Basics & Labeling
- **Äá»c tá»« PDF**: 
  - Náº¿u cÃ³ chÆ°Æ¡ng Detection â†’ Ä‘á»c IoU, mAP, Anchor boxes
  - Náº¿u khÃ´ng cÃ³ â†’ Google search

- **Äá»c thÃªm**:
  - `YOLO object detection explained`
  - `YOLOv8 Ultralytics tutorial`
  - `LabelImg annotation tool`
  - `COCO dataset format`

- **Key concepts**:
  - **Object Detection**: Classify + Localize (bounding box)
  - **IoU (Intersection over Union)**: Metric Ä‘o overlap giá»¯a predicted box vÃ  ground truth
    - Formula: `IoU = Area(overlap) / Area(union)`
    - IoU > 0.5 â†’ good detection
  - **mAP (mean Average Precision)**: Metric tá»•ng há»£p cho detection (mAP@0.5, mAP@0.5:0.95)
  - **YOLO**: Single-stage detector (nhanh), chia áº£nh thÃ nh grid, má»—i cell dá»± Ä‘oÃ¡n bounding box + class
  - **Anchor boxes**: Predefined bounding box shapes (há»c tá»« dataset)

- **Code (5-6h)**:
  - Label ~50-100 áº£nh báº±ng LabelImg (format YOLO txt)
  - Train YOLOv8/v11/RT-DETR (syntax giá»‘ng há»‡t nhau):
  ```python
  from ultralytics import YOLO
  
  # YOLOv8 - Baseline
  model = YOLO('yolov8n.pt')  # nano model (nháº¹, nhanh)
  
  # YOLOv11 - More accurate (recommended náº¿u muá»‘n Ä‘iá»ƒm cao)
  model = YOLO('yolo11n.pt')  # hoáº·c yolo11s.pt
  
  # RT-DETR - Fastest inference (transformer-based)
  model = YOLO('rtdetr-l.pt')  # hoáº·c rtdetr-x.pt
  
  # Training (cÃ¹ng syntax cho cáº£ 3 models)
  model.train(data='data.yaml', epochs=50, imgsz=640, batch=8)
  ```

- **Output**: Model detect bounding box cá»§a khuyáº¿t táº­t, mAP@0.5 ~60-70% (tÃ¹y data quality)

#### Pháº§n 2: Model Comparison & Optimization
- **Äá»c thÃªm**:
  - `YOLO vs Faster R-CNN comparison`
  - `Model quantization INT8 FP16`
  - `ONNX export inference speed`

- **Key concepts**:
  - **YOLOv8**: Nhanh (real-time), accuracy trung bÃ¬nh, model nhá» (3-11MB)
  - **YOLOv11**: Giá»‘ng YOLOv8 nhÆ°ng mAP cao hÆ¡n ~5%, model size tÄƒng nháº¹ (5-12MB)
  - **RT-DETR**: Transformer-based, fastest inference (~2x faster GPU), nhÆ°ng model lá»›n hÆ¡n (20MB)
  - **Inference speed**: FPS (frames per second) trÃªn CPU/GPU
  - **Trade-offs**: YOLOv8 (balance), YOLOv11 (accuracy), RT-DETR (speed)

- **Code (4-5h)**:
  - So sÃ¡nh YOLOv8n vs YOLOv8s (baseline)
  - Náº¿u dÆ° thá»i gian: thÃªm YOLOv11n, RT-DETR-L
  - Export to ONNX:
  ```python
  model.export(format='onnx')  # for deployment
  
  # Benchmark inference speed
  import time
  img = cv2.imread('test.jpg')
  start = time.time()
  results = model(img)
  fps = 1 / (time.time() - start)
  print(f"FPS: {fps:.2f}")
  ```
  - Test inference speed (CPU/GPU)

- **Output**: BÃ¡o cÃ¡o so sÃ¡nh (mAP, FPS, model size), chá»n model phÃ¹ há»£p

**ğŸ“Œ Keywords giai Ä‘oáº¡n 3**: `YOLO`, `YOLOv11`, `RT-DETR`, `Object Detection`, `IoU`, `mAP`, `Bounding Box`, `Anchor-free`, `Transformer Detection`, `LabelImg`, `ONNX`

---

### **Giai Ä‘oáº¡n 4: Semantic Segmentation (NÃ¢ng cao - phÃ¢n Ä‘oáº¡n pixel-level)**

**Má»¥c tiÃªu**: DÃ¹ng U-Net Ä‘á»ƒ phÃ¢n Ä‘oáº¡n khuyáº¿t táº­t (chÃ­nh xÃ¡c hÆ¡n bounding box)

- **Äá»c thÃªm**:
  - `U-Net architecture explained`
  - `Semantic Segmentation vs Instance Segmentation`
  - `Dice Loss IoU metric segmentation`

- **Key concepts**:
  - **Semantic Segmentation**: Classify tá»«ng pixel (background vs defect)
  - **U-Net**: Encoder-Decoder architecture vá»›i skip connections (tá»‘t cho medical/industrial images)
  - **Dice Loss**: Loss function cho segmentation (xá»­ lÃ½ class imbalance tá»‘t)
    - Formula: `Dice = 2Ã—|Aâˆ©B| / (|A|+|B|)`
  - **IoU/Dice score**: Metric Ä‘Ã¡nh giÃ¡ segmentation

- **Code (5-6h)**:
  ```python
  # u_net.py (simplified)
  import segmentation_models_pytorch as smp
  
  # U-Net - Standard choice (best accuracy)
  model = smp.Unet(
      encoder_name="resnet34",
      encoder_weights="imagenet",
      in_channels=1,  # grayscale
      classes=1,       # binary segmentation
  )
  
  # DeepLabV3+ - Faster alternative (náº¿u dÆ° thá»i gian)
  model = smp.DeepLabV3Plus(
      encoder_name="resnet50",       # hoáº·c mobilenet_v2 (fastest)
      encoder_weights="imagenet",
      in_channels=1,
      classes=1
  )
  
  loss = smp.losses.DiceLoss(mode='binary')
  
  # Comparison: U-Net vs DeepLabV3+
  # - U-Net: Dice ~0.78-0.85, inference ~200-400ms (CPU)
  # - DeepLabV3+: Dice ~0.75-0.82, inference ~150-300ms (CPU)
  # Trade-off: DeepLabV3+ 30-40% faster, 2-3% Dice drop
  ```

- **Output**: Mask phÃ¢n Ä‘oáº¡n chÃ­nh xÃ¡c vÃ¹ng khuyáº¿t táº­t (pixel-level), Dice score ~0.75-0.85

**ğŸ“Œ Keywords giai Ä‘oáº¡n 4**: `U-Net`, `DeepLabV3+`, `Semantic Segmentation`, `Dice Loss`, `Pixel-wise Classification`, `Encoder-Decoder`, `ASPP`

---



---

## ğŸ“Š Tá»•ng káº¿t cÃ¡c mÃ´ hÃ¬nh cáº§n tÃ¬m hiá»ƒu (theo thá»© tá»± dá»… â†’ khÃ³)

| Giai Ä‘oáº¡n | MÃ´ hÃ¬nh/Ká»¹ thuáº­t | Má»¥c Ä‘Ã­ch | Äá»™ khÃ³ |
|------|------------------|----------|--------|
| 1 | Classical CV (CLAHE, Morphology) | Preprocessing | â­ |
| 2 | **ResNet/MobileNet** (Transfer Learning) | Image Classification | â­â­ |
| 3 | **YOLOv8** (Object Detection) | Detect bounding box | â­â­â­ |
| 3 (optional) | **YOLOv11 / RT-DETR** | More accurate / Faster detection | â­â­â­ |
| 4 | **U-Net** (Semantic Segmentation) | PhÃ¢n Ä‘oáº¡n pixel-level | â­â­â­â­ |
| 4 (optional) | **DeepLabV3+** | Faster segmentation | â­â­â­â­ |

---

## ğŸ”‘ Key Concepts cáº§n master (Google search keywords)

### Week 1-2 (Classical CV)
- `CLAHE contrast enhancement`
- `Otsu vs Adaptive Threshold`
- `Morphological operations OpenCV`
- `Contour features aspect ratio circularity`

### Week 3-4 (CNN Basics)
- `Convolution explained`
- `Receptive field CNN`
- `Transfer Learning fine-tuning`
- `Data Augmentation techniques`
- `Dropout BatchNorm Regularization`
- `Learning rate scheduler PyTorch`

### Week 5-6 (Object Detection)
- `YOLO architecture how it works`
- `IoU calculation object detection`
- `mAP metric explained`
- `Anchor boxes YOLO`
- `Non-Maximum Suppression NMS`

### Week 7 (Segmentation)
- `U-Net architecture skip connections`
- `Dice Loss vs BCE Loss`
- `Semantic vs Instance Segmentation`

### Week 8 (Deployment)
- `ONNX model export`
- `Model quantization FP16 INT8`
- `FastAPI machine learning tutorial`
- `Docker containerize ML model`

---

## ğŸ’¡ Tips Ä‘á»ƒ khÃ´ng bá»‹ ngá»™p

1. **Má»—i giai Ä‘oáº¡n chá»‰ focus 1 topic chÃ­nh** (vÃ­ dá»¥: CNN basics, Ä‘á»«ng nháº£y sang YOLO ngay)
2. **Code ngay sau khi Ä‘á»c lÃ½ thuyáº¿t** â€” há»c tá»«ng pháº§n nhá»
3. **LÆ°u code + notes vÃ o Git** â€” commit thÆ°á»ng xuyÃªn Ä‘á»ƒ theo dÃµi progress
4. **Äá»c PDF chÆ°Æ¡ng tÆ°Æ¡ng á»©ng trÆ°á»›c, sau Ä‘Ã³ Google search chi tiáº¿t**
5. **Æ¯u tiÃªn practical (code) hÆ¡n theory sÃ¢u** (vÃ­ dá»¥: hiá»ƒu cÃ¡ch dÃ¹ng YOLO > hiá»ƒu toÃ¡n Ä‘áº±ng sau YOLO)
6. **KhÃ´ng cáº§n lÃ m theo thá»© tá»± cá»©ng nháº¯c** â€” nháº£y giai Ä‘oáº¡n náº¿u cáº§n thiáº¿t cho project

---

## ğŸ¯ Deliverables cuá»‘i cÃ¹ng

1. âœ… **Preprocessing pipeline** nÃ¢ng cáº¥p (CLAHE + Adaptive Threshold + Morphology)
2. âœ… **Classifier** (ResNet/MobileNet) phÃ¢n loáº¡i OK/NG vá»›i accuracy >85%
3. âœ… **Detector** (YOLOv8) phÃ¡t hiá»‡n bounding box vá»›i mAP@0.5 >70%
4. âœ… **(Optional)** **Segmentation model** (U-Net) vá»›i Dice >0.75
5. âœ… **API deployment** (FastAPI) + Docker container
6. âœ… **BÃ¡o cÃ¡o so sÃ¡nh** cÃ¡c mÃ´ hÃ¬nh (accuracy, speed, size)

---

## ğŸ¥ á»¨ng dá»¥ng cuá»‘i ká»³: X-ray Defect Detection System

**YÃªu cáº§u tá»•ng há»£p** (tÃ­ch há»£p táº¥t cáº£ kiáº¿n thá»©c tá»« 8 tuáº§n):

### TÃ­nh nÄƒng chÃ­nh
- âœ… **Nháº­n áº£nh X-ray Ä‘áº§u vÃ o** (upload qua web UI hoáº·c API)
- âœ… **PhÃ¡t hiá»‡n vÃ¹ng khuyáº¿t táº­t** báº±ng **YOLOv8** (bounding box)
- âœ… **PhÃ¢n Ä‘oáº¡n vÃ¹ng khuyáº¿t táº­t** báº±ng **U-Net** (pixel-level mask)
- âœ… **TÃ­nh toÃ¡n tá»· lá»‡ % khuyáº¿t táº­t**:
  ```python
  defect_ratio = (sá»‘ pixel khuyáº¿t táº­t / tá»•ng sá»‘ pixel ROI) Ã— 100%
  ```
- âœ… **Hiá»ƒn thá»‹ káº¿t quáº£** qua giao diá»‡n Ä‘Æ¡n giáº£n:
  - Input image (original)
  - YOLOv8 detection (bounding boxes + confidence scores)
  - U-Net segmentation (overlay mask mÃ u Ä‘á»/vÃ ng)
  - Metrics: % khuyáº¿t táº­t, sá»‘ lÆ°á»£ng defects, inference time

### Kiáº¿n trÃºc há»‡ thá»‘ng
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚ â”€â”€â”€â–º â”‚   Backend    â”‚ â”€â”€â”€â–º â”‚   Models    â”‚
â”‚  (Streamlit â”‚      â”‚  (FastAPI)   â”‚      â”‚ YOLOv8+UNet â”‚
â”‚   hoáº·c      â”‚ â—„â”€â”€â”€ â”‚              â”‚ â—„â”€â”€â”€ â”‚             â”‚
â”‚   Gradio)   â”‚      â”‚  Inference   â”‚      â”‚   ONNX      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Implementation Plan

#### 1. Model Training & Export
```python
# 1. Train YOLOv8 (detection)
from ultralytics import YOLO
model_yolo = YOLO('yolov8n.pt')
model_yolo.train(data='xray_defect.yaml', epochs=50, imgsz=640)
model_yolo.export(format='onnx')  # â†’ best_yolo.onnx

# 2. Train U-Net (segmentation)
import segmentation_models_pytorch as smp
model_unet = smp.Unet(encoder_name="resnet34", classes=1)
# ... training loop ...
torch.onnx.export(model_unet, dummy_input, 'unet.onnx')  # â†’ unet.onnx
```

#### 2. Inference Pipeline
```python
# inference.py
import cv2
import numpy as np
import onnxruntime as ort

class XrayDefectDetector:
    def __init__(self, yolo_path, unet_path):
        self.yolo_session = ort.InferenceSession(yolo_path)
        self.unet_session = ort.InferenceSession(unet_path)
    
    def detect_and_segment(self, image):
        # Step 1: YOLOv8 detection
        boxes, scores = self.run_yolo(image)
        
        # Step 2: U-Net segmentation (crop ROI tá»« YOLO boxes)
        masks = []
        for box in boxes:
            x1, y1, x2, y2 = box
            roi = image[y1:y2, x1:x2]
            mask = self.run_unet(roi)
            masks.append(mask)
        
        # Step 3: Calculate defect ratio
        total_defect_pixels = sum([mask.sum() for mask in masks])
        total_roi_pixels = sum([(x2-x1)*(y2-y1) for x1,y1,x2,y2 in boxes])
        defect_ratio = (total_defect_pixels / total_roi_pixels) * 100 if total_roi_pixels > 0 else 0
        
        return {
            'boxes': boxes,
            'scores': scores,
            'masks': masks,
            'defect_ratio': defect_ratio,
            'num_defects': len(boxes)
        }
```

#### 3. API Backend (FastAPI)
```python
# api.py
from fastapi import FastAPI, UploadFile, File
import cv2
import numpy as np
from inference import XrayDefectDetector

app = FastAPI()
detector = XrayDefectDetector('best_yolo.onnx', 'unet.onnx')

@app.post("/predict")
async def predict(file: UploadFile = File(...)):
    # Read image
    img_bytes = await file.read()
    nparr = np.frombuffer(img_bytes, np.uint8)
    img = cv2.imdecode(nparr, cv2.IMREAD_GRAYSCALE)
    
    # Run inference
    results = detector.detect_and_segment(img)
    
    # Visualize
    vis_img = visualize_results(img, results)
    
    return {
        "num_defects": results['num_defects'],
        "defect_ratio": f"{results['defect_ratio']:.2f}%",
        "boxes": results['boxes'].tolist(),
        "scores": results['scores'].tolist(),
        "visualization": encode_image_base64(vis_img)
    }
```

#### 4. Frontend UI (Streamlit hoáº·c Gradio)
```python
# app_streamlit.py
import streamlit as st
import requests
from PIL import Image

st.title("X-ray Defect Detection System")

uploaded_file = st.file_uploader("Upload X-ray image", type=['png', 'jpg'])

if uploaded_file is not None:
    # Display original image
    image = Image.open(uploaded_file)
    st.image(image, caption='Original X-ray', use_column_width=True)
    
    # Send to API
    files = {'file': uploaded_file.getvalue()}
    response = requests.post('http://localhost:8000/predict', files=files)
    results = response.json()
    
    # Display results
    st.subheader("Detection Results")
    col1, col2, col3 = st.columns(3)
    col1.metric("Defects Found", results['num_defects'])
    col2.metric("Defect Ratio", results['defect_ratio'])
    col3.metric("Confidence", f"{max(results['scores'])*100:.1f}%")
    
    # Display visualization
    st.image(results['visualization'], caption='YOLOv8 + U-Net Results', use_column_width=True)
```

### BÃ¡o cÃ¡o so sÃ¡nh YOLOv8 vs U-Net

#### TiÃªu chÃ­ Ä‘Ã¡nh giÃ¡

| TiÃªu chÃ­ | YOLOv8 | YOLOv11 | RT-DETR | U-Net | DeepLabV3+ | Ghi chÃº |
|----------|---------|---------|---------|-------|------------|---------|
| **Má»¥c Ä‘Ã­ch** | Bounding box | Bounding box | Bounding box | Pixel-level | Pixel-level | - |
| **Äá»™ chÃ­nh xÃ¡c** | mAP@0.5: 70-80% | mAP@0.5: 75-85% | mAP@0.5: 72-82% | Dice: 0.75-0.85 | Dice: 0.73-0.83 | YOLOv11 accurate nháº¥t detection |
| **Tá»‘c Ä‘á»™ (CPU)** | ~50-100ms | ~50-100ms | ~30-60ms | ~200-400ms | ~150-300ms | RT-DETR nhanh nháº¥t |
| **Tá»‘c Ä‘á»™ (GPU)** | ~10-20ms | ~10-20ms | ~5-10ms | ~30-50ms | ~20-35ms | RT-DETR fastest inference |
| **Model size** | 3MB (nano) | 5MB (nano) | 20MB (L) | 20-50MB | 25-60MB | YOLO nhá» nháº¥t |
| **Ease of use** | âœ… Ráº¥t dá»… | âœ… Ráº¥t dá»… | âœ… Ráº¥t dá»… | â­â­ KhÃ¡ | â­â­ KhÃ¡ | Detection models dá»… hÆ¡n |
| **Use case** | Screening nhanh | Accuracy cao | Real-time | TÃ­nh % defect | Fast segmentation | TÃ¹y yÃªu cáº§u |

#### Æ¯u Ä‘iá»ƒm

**Detection Models (YOLOv8/v11/RT-DETR)**:
- âœ… Ráº¥t nhanh (real-time trÃªn GPU)
- âœ… Model nhá» gá»n (YOLOv8: 3MB, YOLOv11: 5MB)
- âœ… Dá»… train (Ã­t data, cÃ¹ng syntax)
- âœ… Tá»‘t cho counting (Ä‘áº¿m sá»‘ lÆ°á»£ng defects)
- âœ… **YOLOv11**: Accurate nháº¥t (mAP cao hÆ¡n YOLOv8 ~5%)
- âœ… **RT-DETR**: Nhanh nháº¥t inference (~2x faster than YOLO)

**Segmentation Models (U-Net/DeepLabV3+)**:
- âœ… ChÃ­nh xÃ¡c pixel-level (tÃ­nh % defect chÃ­nh xÃ¡c)
- âœ… PhÃ¢n Ä‘oáº¡n biÃªn rÃµ rÃ ng
- âœ… Tá»‘t cho medical/industrial images
- âœ… CÃ³ thá»ƒ phÃ¢n biá»‡t defects chá»“ng láº¥n
- âœ… **DeepLabV3+**: Nhanh hÆ¡n U-Net ~30-40%, pretrained encoders tá»‘t

#### NhÆ°á»£c Ä‘iá»ƒm

**Detection Models (YOLOv8/v11/RT-DETR)**:
- âŒ Chá»‰ bounding box (khÃ´ng chÃ­nh xÃ¡c vá» diá»‡n tÃ­ch)
- âŒ KhÃ³ phÃ¢n Ä‘oáº¡n defects cÃ³ hÃ¬nh dáº¡ng phá»©c táº¡p
- âŒ Box overlap khi defects gáº§n nhau
- âŒ **RT-DETR**: Model size lá»›n hÆ¡n YOLO (~20MB vs 3-5MB)

**Segmentation Models (U-Net/DeepLabV3+)**:
- âŒ Cháº­m hÆ¡n detection (2-4x)
- âŒ Model lá»›n hÆ¡n (~25-60MB)
- âŒ Cáº§n nhiá»u data labeled (pixel-wise masks)
- âŒ KhÃ³ train vá»›i small dataset
- âŒ **DeepLabV3+**: Dice score tháº¥p hÆ¡n U-Net ~2-3% (trade-off speed vs accuracy)

#### Káº¿t luáº­n & Khuyáº¿n nghá»‹

**Chiáº¿n lÆ°á»£c káº¿t há»£p (best of both worlds)**:
1. **Stage 1 (Detection)**: YOLOv8/v11/RT-DETR â†’ detect ROI nhanh
2. **Stage 2 (Segmentation)**: U-Net/DeepLabV3+ â†’ segment chi tiáº¿t trong ROI
3. **Lá»£i Ã­ch**: Tá»‘c Ä‘á»™ detection + Ä‘á»™ chÃ­nh xÃ¡c segmentation

**Chá»n model phÃ¹ há»£p**:
- **YOLOv8**: Baseline tá»‘t, nhá» gá»n (recommended báº¯t Ä‘áº§u)
- **YOLOv11**: Accuracy cao nháº¥t detection (+5% mAP) - náº¿u muá»‘n Ä‘iá»ƒm cao
- **RT-DETR**: Fastest inference (real-time edge devices)
- **U-Net**: Accuracy cao nháº¥t segmentation (standard choice)
- **DeepLabV3+**: Faster segmentation, trade-off 2-3% Dice cho 30-40% speed

**Äá» xuáº¥t cho bÃ¡o cÃ¡o (tÄƒng Ä‘iá»ƒm)**:
- **Minimum (pass)**: YOLOv8 + U-Net
- **Better (Ä‘iá»ƒm cao)**: ThÃªm YOLOv11 hoáº·c RT-DETR + so sÃ¡nh detection models
- **Best (Ä‘iá»ƒm ráº¥t cao)**: Train cáº£ 4-5 models â†’ comparative analysis table vá»›i mAP/Dice/FPS/Size

---

---

## ğŸ”§ Giai Ä‘oáº¡n 5 (Cuá»‘i cÃ¹ng): UI & Deployment

**LÃ m sau khi Ä‘Ã£ train xong model detection/segmentation**

### Model Export & Optimization

- **Äá»c thÃªm**:
  - `ONNX model export PyTorch`
  - `Model quantization FP16 INT8`
  - `TensorRT OpenVINO optimization`

- **Key concepts**:
  - **Model export**: `.pt` (PyTorch) â†’ `.onnx` (cross-framework) â†’ `.engine` (TensorRT)
  - **Inference optimization**: 
    - Quantization (FP32 â†’ FP16/INT8) â†’ 2-4Ã— faster
    - Batch inference (process multiple images at once)
    - OpenVINO (Intel) / TensorRT (NVIDIA) for hardware acceleration

- **Code**:
  ```python
  # Export YOLO to ONNX
  from ultralytics import YOLO
  model = YOLO('best.pt')
  model.export(format='onnx')  # â†’ best.onnx
  
  # Export U-Net to ONNX
  import torch
  torch.onnx.export(unet_model, dummy_input, 'unet.onnx')
  ```

**ğŸ“Œ Keywords**: `ONNX`, `TensorRT`, `OpenVINO`, `Quantization`, `Model Export`

---

### API Backend (FastAPI)

- **Äá»c thÃªm**:
  - `FastAPI machine learning deployment`
  - `Docker containerization ML model`

- **Code**:
  ```python
  # api.py
  from fastapi import FastAPI, UploadFile
  import cv2
  import numpy as np
  from ultralytics import YOLO
  
  app = FastAPI()
  model = YOLO('best.pt')
  
  @app.post("/predict")
  async def predict(file: UploadFile):
      img = cv2.imdecode(np.frombuffer(await file.read(), np.uint8), cv2.IMREAD_COLOR)
      results = model(img)
      return {"boxes": results[0].boxes.xyxy.tolist()}
  ```

**ğŸ“Œ Keywords**: `FastAPI`, `Docker`, `API Deployment`

---

### Frontend UI (Streamlit/Gradio)

- **Code máº«u** (Streamlit):
  ```python
  # app_streamlit.py
  import streamlit as st
  import requests
  from PIL import Image
  
  st.title("X-ray Defect Detection")
  uploaded = st.file_uploader("Upload X-ray", type=['png', 'jpg'])
  
  if uploaded:
      image = Image.open(uploaded)
      st.image(image, caption='Original', use_column_width=True)
      
      # Send to API
      files = {'file': uploaded.getvalue()}
      response = requests.post('http://localhost:8000/predict', files=files)
      results = response.json()
      
      st.metric("Defects Found", results['num_defects'])
      st.metric("Defect Ratio", results['defect_ratio'])
  ```

**ğŸ“Œ Keywords**: `Streamlit`, `Gradio`, `Web UI`

---

## ğŸ“‹ Checklist hoÃ n thÃ nh project

### Core (Æ¯u tiÃªn - Ä‘á»§ Ä‘á»ƒ pass)
- [ ] **Giai Ä‘oáº¡n 1-2**: Preprocessing + Classification (baseline)
- [ ] **Giai Ä‘oáº¡n 3**: YOLOv8 training + evaluation (mAP >70%)
- [ ] **Giai Ä‘oáº¡n 4**: U-Net training + evaluation (Dice >0.75)
- [ ] **Inference pipeline**: YOLO â†’ U-Net (script Ä‘Ã£ cÃ³: `scripts/inference_pipeline.py`)
- [ ] **BÃ¡o cÃ¡o so sÃ¡nh**: YOLOv8 vs U-Net (accuracy, speed, model size)

### Advanced (Náº¿u dÆ° thá»i gian - Ä‘á»ƒ Ä‘iá»ƒm cao hÆ¡n)
- [ ] **Detection comparison**: Train thÃªm YOLOv11 hoáº·c RT-DETR â†’ compare vá»›i YOLOv8
- [ ] **Segmentation comparison**: Train thÃªm DeepLabV3+ â†’ compare vá»›i U-Net
- [ ] **Comparative analysis**: Table so sÃ¡nh mAP/Dice/FPS/Model Size cá»§a 4-5 models
- [ ] **Ablation study**: Test cÃ¡c encoder backbones khÃ¡c nhau (ResNet34 vs MobileNetV2 vs EfficientNet)

### Polish (LÃ m sau)
- [ ] **Model export**: Convert to ONNX/TensorRT
- [ ] **API backend**: FastAPI serve model
- [ ] **Frontend UI**: Streamlit/Gradio
- [ ] **Docker deployment**: Container hÃ³a app
- [ ] **Demo video**: Upload áº£nh â†’ hiá»ƒn thá»‹ káº¿t quáº£

---

**Next step**: Báº¯t Ä‘áº§u giai Ä‘oáº¡n 1 â†’ upgrade `test02.py` vá»›i CLAHE + Adaptive Threshold. Hoáº·c nháº£y tháº³ng sang giai Ä‘oáº¡n 3-4 náº¿u muá»‘n train YOLO/U-Net trÆ°á»›c (recommended: focus detection/segmentation trÆ°á»›c, UI sau).
